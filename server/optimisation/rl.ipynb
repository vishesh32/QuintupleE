{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Algo\n",
    "\n",
    "State\n",
    "deferable_demand * 3\n",
    "flywheel_amt\n",
    "1. demand(inst): float\n",
    "2. sun: int (percentage)\n",
    "2. buy_price: int (cents/joule)\n",
    "3. sell_price: int (cents/joule)\n",
    "5. stacked, use last X values\n",
    "\n",
    "Action\n",
    "1. import_export_amount (in joules): positive for import and negative for export\n",
    "2. release_store_amount (in joules): positive for release and negative for store\n",
    "3. for each deferrable demand: energy to allocate for that time tick as a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from gen_data import getTicksForDay\n",
    "from optimisation.utils.gen_utils import  get_ema\n",
    "\n",
    "\n",
    "from policy import PolicyNetwork, ValueNetwork\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from naive import simulate_day_naive\n",
    "from algorithm import cur_tick_to_vect, future_ticks_to_vect, history_ticks_to_vect, compute_loss, compute_returns, environment_step, load_policy_network_checkpoint\n",
    "from algorithm import ALLOCATION_MULTIPLIER, ALLOCATION_ABSOLUTE, STACKED_NUM, STATE_SIZE, ACTION_SIZE, TICK_LENGTH, RELEASE_STORE_MULTIPLIER, FUTURE_TICKS, IMPORT_EXPORT_MULTIPLIER\n",
    "from train_test_utils import plot_test_results\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Get number of files in checkpoints directory\n",
    "def get_num_files():\n",
    "\n",
    "    files = os.listdir(\"checkpoints\")\n",
    "\n",
    "    return str(len(files) + 1).zfill(2)\n",
    "    # return len([name for name in os.listdir('checkpoints') if os.path.isfile(name)])\n",
    "\n",
    "# REWARD_SCALING_FACTOR = 1\n",
    "REWARD_SCALING_FACTOR = 1 / 100\n",
    "\n",
    "def run_simulation(ticks, history_ticks, day, print_info=False):\n",
    "\n",
    "    day_state = {\n",
    "        \"deferables\": deepcopy(day.deferables),\n",
    "        \"flywheel_amt\": 0\n",
    "    }\n",
    "\n",
    "    # print(\"Deferables: \", [[d.start, d.end] for d in day_state[\"deferables\"]])\n",
    "\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    costs = []\n",
    "    penalties = []\n",
    "    \n",
    "    for i, tick in enumerate(ticks):\n",
    "        state = []\n",
    "\n",
    "        # Add deferable demand info\n",
    "        deferables = day_state[\"deferables\"]\n",
    "        for j in range(len(deferables)):\n",
    "            d = deferables[j]\n",
    "            state.extend([d.energy, d.start, d.end])\n",
    "        \n",
    "        # Add store/release info\n",
    "        state.append(day_state[\"flywheel_amt\"])\n",
    "        \n",
    "        # Add tick info\n",
    "        state.extend(cur_tick_to_vect(tick))\n",
    "        state.extend(history_ticks_to_vect(history_ticks, STACKED_NUM))\n",
    "        if FUTURE_TICKS > 0:\n",
    "            state.extend(future_ticks_to_vect(history_ticks + [tick]))\n",
    "\n",
    "        # Run policy network\n",
    "        # TODO: Batch this?\n",
    "        action, log_prob = policy_network.get_action(torch.tensor(state))\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Environment step\n",
    "        cost, penalty, _ = environment_step(action, tick, day_state, print_info=print_info)\n",
    "        penalties.append(penalty)\n",
    "        costs.append(cost)\n",
    "        rewards.append(-(cost + penalty))\n",
    "        states.append(state)\n",
    "\n",
    "        history_ticks.append(tick)\n",
    "        history_ticks.pop(0)\n",
    "\n",
    "\n",
    "    return log_probs, rewards, costs, penalties, states\n",
    "\n",
    "def simulate_day(day, ticks, history_ticks, runs_per_trajectory, print_info=False):\n",
    "    global max_returns\n",
    "    \n",
    "    # Run simulation\n",
    "    total_loss = 0\n",
    "    trajectories = []\n",
    "\n",
    "    # TODO: Batch this\n",
    "    for i in range(runs_per_trajectory):\n",
    "        log_probs, rewards, costs, penalties, states = run_simulation(ticks, history_ticks, day, print_info=print_info)\n",
    "        trajectories.append((sum(rewards), sum(costs), sum(penalties)))\n",
    "        rewards = torch.tensor(rewards)\n",
    "        rewards = rewards * REWARD_SCALING_FACTOR\n",
    "\n",
    "        returns = compute_returns(rewards)\n",
    "        total_loss += compute_loss(log_probs, returns)\n",
    "\n",
    "\n",
    "    average_loss = total_loss / RUNS_PER_TRAJECTORY\n",
    "    policy_network.optimizer.zero_grad()\n",
    "    average_loss.backward()\n",
    "    policy_network.optimizer.step()\n",
    "\n",
    "    return {\n",
    "        \"loss\": average_loss.item(),\n",
    "        \"reward\": sum([r[0] for r in trajectories]) / len(trajectories),\n",
    "        \"cost\": sum([r[1] for r in trajectories]) / len(trajectories),\n",
    "        \"penalty\": sum([r[2] for r in trajectories]) / len(trajectories)\n",
    "    }\n",
    "\n",
    "def train(naive_params, policy_network, num_epochs, runs_per_trajectory, filename):\n",
    "    \n",
    "    last_checkpoint = None\n",
    "    history_ticks = [None] * STACKED_NUM\n",
    "    min_cost = float('inf')\n",
    "    \n",
    "    results = []\n",
    "    naive_costs = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        day, ticks = getTicksForDay(i)\n",
    "\n",
    "        last_checkpoint = {\n",
    "            \"mean_net\": policy_network.mean_net.state_dict().copy(),\n",
    "            \"logstd\": policy_network.logstd.clone(),\n",
    "        }\n",
    "\n",
    "        r = simulate_day(day, ticks, history_ticks, runs_per_trajectory, print_info=False)\n",
    "        results.append(r)\n",
    "        emas = get_ema([r[\"cost\"] for r in results], 100)\n",
    "        \n",
    "        naive_costs.append(simulate_day_naive(day, ticks, satisfy_end=naive_params[\"satisfy_end\"], use_flywheel=naive_params[\"use_flywheel\"], export_extra=not naive_params[\"use_flywheel\"]))\n",
    "\n",
    "        if last_checkpoint != None and emas[-1] < min_cost:\n",
    "            min_cost = emas[-1]\n",
    "            last_checkpoint[\"min\"] = emas[-1]\n",
    "            last_checkpoint[\"min_index\"] = i\n",
    "            torch.save(last_checkpoint, filename)\n",
    "            \n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(\"Cost:\", round(r[\"cost\"], 3))\n",
    "            print(\"Cost EMA (100): \", emas[-1])\n",
    "            # cur_time = time.time()\n",
    "            # print(\"Pen:\", round(r[\"penalty\"], 3))\n",
    "            # print(\"Reward\", round(r[\"reward\"], 3))\n",
    "            # print(\"Reward EMA (100): \", get_ema([r[\"reward\"] for r in results], 100)[-1])\n",
    "            # print(f\"Time taken: {round(cur_time - start_time, 2)}s\")\n",
    "            print('-' * 20)\n",
    "            print()\n",
    "\n",
    "    return results, naive_costs\n",
    "    \n",
    "\n",
    "RUNS_PER_TRAJECTORY = 100\n",
    "NUM_EPOCHS = 5000\n",
    "num_files = get_num_files()\n",
    "basename = f\"{num_files}_e{NUM_EPOCHS}_r{RUNS_PER_TRAJECTORY}_am{ALLOCATION_MULTIPLIER}_rm{RELEASE_STORE_MULTIPLIER}_im{IMPORT_EXPORT_MULTIPLIER}_sta{STACKED_NUM}_abs{ALLOCATION_ABSOLUTE}\"\n",
    "\n",
    "filename = f\"checkpoints/{basename}.pth\"\n",
    "naive_params = {\"use_flywheel\": True, \"satisfy_end\": False}\n",
    "\n",
    "policy_network = PolicyNetwork(state_size=STATE_SIZE, action_size=ACTION_SIZE, training=True)\n",
    "results, naive_costs = train(naive_params, policy_network, NUM_EPOCHS, RUNS_PER_TRAJECTORY, filename)\n",
    "# policy_network = load_policy_network_checkpoint(filename)\n",
    "\n",
    "# Plotting\n",
    "plot_test_results(results, naive_costs, naive_params, basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_md = costs_to_table_md([\n",
    "#     (\"RL\", rl_costs),\n",
    "#     (\"Naive (EE) End\", naive_ee_end),\n",
    "# ])\n",
    "# display(Markdown(table_md))\n",
    "\n",
    "from random import randint\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "\n",
    "from optimisation.utils.gen_utils import get_ema\n",
    "from optimisation.train_test_utils import get_naive_label, run_validation\n",
    "from optimisation.naive import simulate_day_naive\n",
    "from optimisation.gen_data import getDayData, getTickData, getTicksForDay\n",
    "from algorithm import STATE_SIZE, ACTION_SIZE, load_policy_network_checkpoint\n",
    "from algorithm import predict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from random import randint\n",
    "\n",
    "basename = \"06_e5000_r50_am2_rm0_im10_sta20_abs1\"\n",
    "# basename = \"04_e2000_r20_am2_rm0_im5_sta20_abs1\"\n",
    "filename = f\"checkpoints/{basename}.pth\"\n",
    "policy_network, min, min_epoch = load_policy_network_checkpoint(filename)\n",
    "\n",
    "start = randint(5000000, 6000000)\n",
    "print(\"Training min: \", round(min, 3))\n",
    "print(\"Min epoch: \", min_epoch)\n",
    "print(\"Start:\", start)\n",
    "print()\n",
    "\n",
    "env = {\n",
    "    \"deferables\": None,\n",
    "    \"flywheel_amt\": 0\n",
    "}\n",
    "\n",
    "history_ticks = []\n",
    "\n",
    "\n",
    "number_of_days = 1000\n",
    "\n",
    "naive_params = {\n",
    "    \"satisfy_end\": False,\n",
    "    \"use_flywheel\": True\n",
    "}\n",
    "\n",
    "rl_costs, naive_costs = run_validation(start, number_of_days, policy_network, naive_params)\n",
    "\n",
    "ema_amount = 100\n",
    "plt.plot(get_ema(rl_costs, ema_amount), label=\"RL\")\n",
    "plt.plot(get_ema(naive_costs, ema_amount), label=get_naive_label(naive_params))\n",
    "\n",
    "mid_y = (max(get_ema(naive_costs, ema_amount)) - max(get_ema(rl_costs, ema_amount)))//2 + max(get_ema(rl_costs, ema_amount))\n",
    "plt.text(0, mid_y, \n",
    "f'''Average RL cost: {round(np.mean(rl_costs), 2)}\n",
    "Average Naive cost: {round(np.mean(naive_costs), 2)}''', \n",
    "         bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost over days (ema=100)\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"plots/{basename}_val.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "from utils import costs_to_table_md\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "from algorithm import simulate_day_naive\n",
    "from gen_data import getTicksForDay\n",
    "from utils import get_ema\n",
    "from random import randint\n",
    "\n",
    "import random\n",
    "\n",
    "start = randint(5000000, 6000000)\n",
    "print(\"Start day: \", start)\n",
    "size = 1000\n",
    "\n",
    "naive_fw_start = []\n",
    "naive_ee_start = []\n",
    "naive_fw_end = []\n",
    "naive_ee_end = []\n",
    "\n",
    "for day_id in range(start, start + size):\n",
    "    day, ticks = getTicksForDay(day_id)\n",
    "    naive_fw_start.append(simulate_day_naive(day, ticks, use_flywheel=True, satisfy_end=False))\n",
    "    naive_ee_start.append(simulate_day_naive(day, ticks, export_extra=True, satisfy_end=False))\n",
    "    naive_fw_end.append(simulate_day_naive(day, ticks, use_flywheel=True, satisfy_end=True))\n",
    "    naive_ee_end.append(simulate_day_naive(day, ticks, export_extra=True, satisfy_end=True))\n",
    "\n",
    "table_md = costs_to_table_md(\n",
    "    [\n",
    "        (\"Naive (FW) Start\", naive_fw_start),\n",
    "        (\"Naive (EE) Start\", naive_ee_start),\n",
    "        (\"Naive (FW) End\", naive_fw_end),\n",
    "        (\"Naive (EE) End\", naive_ee_end),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(Markdown(table_md))\n",
    "plt.plot(get_ema(naive_fw_start, 100), label=\"Naive (FW) Start\")\n",
    "plt.plot(get_ema(naive_ee_start, 100), label=\"Naive (EE) Start\")\n",
    "plt.plot(get_ema(naive_fw_end, 100), label=\"Naive (FW) End\")\n",
    "plt.plot(get_ema(naive_ee_end, 100), label=\"Naive (EE) End\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Costs over days (ema=100)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartgrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
