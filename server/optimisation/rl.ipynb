{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Algo\n",
    "\n",
    "State\n",
    "deferable_demand * 3\n",
    "flywheel_amt\n",
    "1. demand(inst): float\n",
    "2. sun: int (percentage)\n",
    "2. buy_price: int (cents/joule)\n",
    "3. sell_price: int (cents/joule)\n",
    "5. stacked, use last X values\n",
    "\n",
    "Action\n",
    "1. import_export_amount (in joules): positive for import and negative for export\n",
    "2. release_store_amount (in joules): positive for release and negative for store\n",
    "3. for each deferrable demand: energy to allocate for that time tick as a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from gen_data import getTicksForDay\n",
    "from policy import PolicyNetwork, ValueNetwork\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import  get_ema\n",
    "\n",
    "from algorithm import cur_tick_to_vect, history_ticks_to_vect, import_export_to_cost, update_deferable_demands, simulate_day_naive, update_flywheel_amt, get_sun_energy, compute_loss, compute_returns, environment_step\n",
    "from algorithm import DEFERABLE_DEADLINE_PENALTY, ALLOCATION_MULTIPLIER, ALLOCATION_ABSOLUTE, STACKED_NUM, STATE_SIZE, ACTION_SIZE\n",
    "from train_test_utils import plot_test_results\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_simulation(ticks, history_ticks, day, print_info=False):\n",
    "\n",
    "    day_state = {\n",
    "        \"deferables\": deepcopy(day.deferables),\n",
    "        \"flywheel_amt\": 0\n",
    "    }\n",
    "\n",
    "    # print(\"Deferables: \", [[d.start, d.end] for d in day_state[\"deferables\"]])\n",
    "\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    costs = []\n",
    "    penalties = []\n",
    "    \n",
    "    for i, tick in enumerate(ticks):\n",
    "        state = []\n",
    "\n",
    "        # Add deferable demand info\n",
    "        deferables = day_state[\"deferables\"]\n",
    "        for j in range(len(deferables)):\n",
    "            d = deferables[j]\n",
    "            state.extend([d.energy, d.start, d.end])\n",
    "        \n",
    "        # Add store/release info\n",
    "        state.append(day_state[\"flywheel_amt\"])\n",
    "        \n",
    "        # Add tick info\n",
    "        state.extend(cur_tick_to_vect(tick))\n",
    "        state.extend(history_ticks_to_vect(history_ticks, STACKED_NUM))\n",
    "\n",
    "        # Run policy network\n",
    "        # TODO: Batch this?\n",
    "        action, log_prob = policy_network.get_action(torch.tensor(state))\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Environment step\n",
    "        cost, penalty, _ = environment_step(action, tick, day_state, print_info=print_info)\n",
    "        penalties.append(penalty)\n",
    "        costs.append(cost)\n",
    "        rewards.append(-(cost + penalty))\n",
    "        states.append(state)\n",
    "\n",
    "        history_ticks.append(tick)\n",
    "        history_ticks.pop(0)\n",
    "\n",
    "\n",
    "    return log_probs, rewards, costs, penalties, states\n",
    "\n",
    "def simulate_day(day, ticks, history_ticks, runs_per_trajectory, print_info=False):\n",
    "    global max_returns\n",
    "    \n",
    "    # Run simulation\n",
    "    total_loss = 0\n",
    "    trajectories = []\n",
    "\n",
    "    # TODO: Batch this\n",
    "    for i in range(runs_per_trajectory):\n",
    "        log_probs, rewards, costs, penalties, states = run_simulation(ticks, history_ticks, day, print_info=print_info)\n",
    "        trajectories.append((sum(rewards), sum(costs), sum(penalties)))\n",
    "        returns = compute_returns(rewards)\n",
    "        total_loss += compute_loss(log_probs, returns)\n",
    "\n",
    "\n",
    "    average_loss = total_loss / RUNS_PER_TRAJECTORY\n",
    "    policy_network.optimizer.zero_grad()\n",
    "    average_loss.backward()\n",
    "    policy_network.optimizer.step()\n",
    "\n",
    "    return {\n",
    "        \"loss\": average_loss.item(),\n",
    "        \"reward\": sum([r[0] for r in trajectories]) / len(trajectories),\n",
    "        \"cost\": sum([r[1] for r in trajectories]) / len(trajectories),\n",
    "        \"penalty\": sum([r[2] for r in trajectories]) / len(trajectories)\n",
    "    }\n",
    "\n",
    "def train(naive_params, policy_network, num_epochs, runs_per_trajectory, filename):\n",
    "    \n",
    "    last_checkpoint = None\n",
    "    history_ticks = [None] * STACKED_NUM\n",
    "    min_cost = 1e6\n",
    "    \n",
    "    results = []\n",
    "    naive_costs = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        day, ticks = getTicksForDay(i)\n",
    "\n",
    "        last_checkpoint = {\n",
    "            \"mean_net\": policy_network.mean_net.state_dict().copy(),\n",
    "            \"logstd\": policy_network.logstd.clone(),\n",
    "        }\n",
    "\n",
    "        r = simulate_day(day, ticks, history_ticks, runs_per_trajectory, print_info=False)\n",
    "        results.append(r)\n",
    "        emas = get_ema([r[\"cost\"] for r in results], 100)\n",
    "        \n",
    "        naive_costs.append(simulate_day_naive(day, ticks, satisfy_end=naive_params[\"satisfy_end\"], use_flywheel=naive_params[\"use_flywheel\"], export_extra=not naive_params[\"use_flywheel\"]))\n",
    "\n",
    "        if last_checkpoint != None and emas[-1] < min_cost:\n",
    "            last_checkpoint[\"min\"] = emas[-1]\n",
    "            last_checkpoint[\"min_index\"] = i\n",
    "            torch.save(last_checkpoint, filename)\n",
    "            \n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(\"Cost:\", round(r[\"cost\"], 3))\n",
    "            print(\"Cost EMA (100): \", emas[-1])\n",
    "            # cur_time = time.time()\n",
    "            # print(\"Pen:\", round(r[\"penalty\"], 3))\n",
    "            # print(\"Reward\", round(r[\"reward\"], 3))\n",
    "            # print(\"Reward EMA (100): \", get_ema([r[\"reward\"] for r in results], 100)[-1])\n",
    "            # print(f\"Time taken: {round(cur_time - start_time, 2)}s\")\n",
    "            print('-' * 20)\n",
    "            print()\n",
    "\n",
    "    return results, naive_costs\n",
    "    \n",
    "\n",
    "RUNS_PER_TRAJECTORY = 20\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "basename = f\"e{NUM_EPOCHS}_r{RUNS_PER_TRAJECTORY}_pen{DEFERABLE_DEADLINE_PENALTY}_mul{ALLOCATION_MULTIPLIER}_abs{ALLOCATION_ABSOLUTE}_sta{STACKED_NUM}\"\n",
    "\n",
    "filename = f\"checkpoints/{basename}.pth\"\n",
    "policy_network = PolicyNetwork(STATE_SIZE, ACTION_SIZE)\n",
    "naive_params = {\"use_flywheel\": False, \"satisfy_end\": True}\n",
    "\n",
    "results, naive_costs = train(naive_params, policy_network, NUM_EPOCHS, RUNS_PER_TRAJECTORY, filename)\n",
    "\n",
    "# Plotting\n",
    "plot_test_results(results, naive_costs, naive_params, basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_md = costs_to_table_md([\n",
    "#     (\"RL\", rl_costs),\n",
    "#     (\"Naive (EE) End\", naive_ee_end),\n",
    "# ])\n",
    "# display(Markdown(table_md))\n",
    "\n",
    "from random import randint\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "\n",
    "from optimisation.train_test_utils import get_naive_label, run_validation\n",
    "from optimisation.algorithm import simulate_day_naive\n",
    "from optimisation.gen_data import getDayData, getTickData, getTicksForDay\n",
    "from algorithm import STATE_SIZE, ACTION_SIZE, load_policy_network_checkpoint\n",
    "from algorithm import predict\n",
    "from utils import costs_to_table_md, get_ema\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "basename = \"e500_r20_pen0_mul1_abs1_sta30\"\n",
    "filename = f\"checkpoints/{basename}.pth\"\n",
    "policy_network, min, min_epoch = load_policy_network_checkpoint(filename)\n",
    "# print(\"Training min: \", round(min, 3))\n",
    "# print(\"Min epoch: \", min_epoch)\n",
    "\n",
    "env = {\n",
    "    \"deferables\": None,\n",
    "    \"flywheel_amt\": 0\n",
    "}\n",
    "\n",
    "history_ticks = []\n",
    "\n",
    "start = 6000000\n",
    "number_of_days = 1000\n",
    "\n",
    "naive_params = {\n",
    "    \"satisfy_end\": True,\n",
    "    \"use_flywheel\": False\n",
    "}\n",
    "\n",
    "rl_costs, naive_costs = run_validation(start, number_of_days, policy_network, naive_params)\n",
    "\n",
    "ema_amount = 100\n",
    "plt.plot(get_ema(rl_costs, ema_amount), label=\"RL\")\n",
    "plt.plot(get_ema(naive_costs, ema_amount), label=get_naive_label(naive_params))\n",
    "\n",
    "mid_y = max(get_ema(naive_costs, ema_amount)) // 2\n",
    "plt.text(0, mid_y, \n",
    "f'''Average RL cost: {round(np.mean(rl_costs), 2)}\n",
    "Average Naive cost: {round(np.mean(naive_costs), 2)}''', \n",
    "         bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost over days (ema=100)\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"plots/{basename}_val.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "from utils import costs_to_table_md\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "from algorithm import simulate_day_naive\n",
    "from gen_data import getTicksForDay\n",
    "from utils import get_ema\n",
    "from random import randint\n",
    "\n",
    "start = randint(5000000, 6000000)\n",
    "print(\"Start day: \", start)\n",
    "size = 1000\n",
    "\n",
    "naive_fw_start = []\n",
    "naive_ee_start = []\n",
    "naive_fw_end = []\n",
    "naive_ee_end = []\n",
    "\n",
    "for day_id in range(start, start + size):\n",
    "    day, ticks = getTicksForDay(day_id)\n",
    "    naive_fw_start.append(simulate_day_naive(day, ticks, use_flywheel=True, satisfy_end=False))\n",
    "    naive_ee_start.append(simulate_day_naive(day, ticks, export_extra=True, satisfy_end=False))\n",
    "    naive_fw_end.append(simulate_day_naive(day, ticks, use_flywheel=True, satisfy_end=True))\n",
    "    naive_ee_end.append(simulate_day_naive(day, ticks, export_extra=True, satisfy_end=True))\n",
    "\n",
    "table_md = costs_to_table_md(\n",
    "    [\n",
    "        (\"Naive (FW) Start\", naive_fw_start),\n",
    "        (\"Naive (EE) Start\", naive_ee_start),\n",
    "        (\"Naive (FW) End\", naive_fw_end),\n",
    "        (\"Naive (EE) End\", naive_ee_end),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(Markdown(table_md))\n",
    "plt.plot(get_ema(naive_fw_start, 100), label=\"Naive (FW) Start\")\n",
    "plt.plot(get_ema(naive_ee_start, 100), label=\"Naive (EE) Start\")\n",
    "plt.plot(get_ema(naive_fw_end, 100), label=\"Naive (FW) End\")\n",
    "plt.plot(get_ema(naive_ee_end, 100), label=\"Naive (EE) End\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Costs over days (ema=100)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def environment_step(action, tick, day_state, print_info=False):\n",
    "#     total_penalty = 0\n",
    "    \n",
    "#     # 1. Get total sun energy\n",
    "#     sun_energy = get_sun_energy(tick)\n",
    "#     total_energy = sun_energy\n",
    "\n",
    "#     # 2. Get energy bought/sold\n",
    "#     imp_exp_amt = action[0].item()\n",
    "#     total_energy += imp_exp_amt\n",
    "\n",
    "#     # 3. Get energy stored/released\n",
    "#     release_store_amt = action[1].item()\n",
    "#     release_store_amt, penalty = update_flywheel_amt(day_state, release_store_amt)\n",
    "#     total_penalty += penalty\n",
    "#     total_energy += release_store_amt\n",
    "\n",
    "#     # 4. Satisfy instantaneous demand\n",
    "#     total_energy -= tick.demand\n",
    "\n",
    "#     # 5. Satisfy deferable demands\n",
    "#     energy_spent, penalty, allocations = update_deferable_demands(day_state, action, tick)\n",
    "#     total_penalty += penalty\n",
    "#     total_energy -= energy_spent\n",
    "\n",
    "#     # 6. Buy more energy if total energy < 0\n",
    "#     if total_energy < 0:\n",
    "#         imp_exp_amt += -total_energy\n",
    "    \n",
    "#     if imp_exp_amt > MAX_IMPORT_EXPORT:\n",
    "#         imp_exp_amt = MAX_IMPORT_EXPORT\n",
    "#     elif imp_exp_amt < -MAX_IMPORT_EXPORT:\n",
    "#         imp_exp_amt = -MAX_IMPORT_EXPORT\n",
    "\n",
    "#     if total_energy > 0:\n",
    "#         if total_energy > MAX_FLYWHEEL_CAPACITY - day_state[\"flywheel_amt\"]:\n",
    "#             energy_to_sell = total_energy - (MAX_FLYWHEEL_CAPACITY - day_state[\"flywheel_amt\"])\n",
    "#             imp_exp_amt -= energy_to_sell\n",
    "\n",
    "#         day_state[\"flywheel_amt\"] = min(MAX_FLYWHEEL_CAPACITY, day_state[\"flywheel_amt\"] + total_energy)\n",
    "\n",
    "#     cost = import_export_to_cost(imp_exp_amt, tick)\n",
    "\n",
    "#     if print_info:\n",
    "#         print(\"Tick: \", tick.tick)\n",
    "#         print(\"Sun E: \", round(sun_energy, 3))\n",
    "#         print(\"Imp/Exp: \", round(imp_exp_amt, 3))\n",
    "#         print(\"Cost: \", round(cost, 5))\n",
    "#         print(\"Rel/Sto: \", round(release_store_amt, 3))\n",
    "#         print(\"Ins D: \", round(tick.demand, 3))\n",
    "#         print(\"Def D: \", round(energy_spent, 3))\n",
    "#         print(\"E left: \", round(total_energy, 3))\n",
    "#         print(\"Penalty: \", round(total_penalty, 3))\n",
    "#         print('-' * 20)\n",
    "#         print()\n",
    "\n",
    "#     return cost, total_penalty\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartgrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
