{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Algo\n",
    "\n",
    "State\n",
    "deferable_demand * 3\n",
    "flywheel_amt\n",
    "1. demand(inst): float\n",
    "2. sun: int (percentage)\n",
    "2. buy_price: int (cents/joule)\n",
    "3. sell_price: int (cents/joule)\n",
    "5. stacked, use last X values\n",
    "\n",
    "Action\n",
    "1. import_export_amount (in joules): positive for import and negative for export\n",
    "2. release_store_amount (in joules): positive for release and negative for store\n",
    "3. for each deferrable demand: energy to allocate for that time tick as a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from models import Day, Tick\n",
    "from utils import get_db\n",
    "from pydantic import BaseModel\n",
    "\n",
    "db = get_db()\n",
    "\n",
    "days = db.days.find().limit(1000).sort(\"day\", 1)\n",
    "days = [Day.model_validate(day) for day in days]\n",
    "id2day = {day.day: day for day in days}\n",
    "\n",
    "ticks = db.ticks.find().limit(60000)\n",
    "ticks = [Tick.model_validate(tick) for tick in ticks]\n",
    "\n",
    "day2ticks = {}\n",
    "for id, tick in enumerate(ticks):\n",
    "    if tick.day not in day2ticks:\n",
    "        day2ticks[tick.day] = []\n",
    "    \n",
    "    day2ticks[tick.day].append(tick)\n",
    "\n",
    "print(\"Num days: \", len(days))\n",
    "print(\"Num ticks: \", len(ticks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from gen_data import getTicksForDay\n",
    "from policy import PolicyNetwork, ValueNetwork\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import  get_ema, print_release_store\n",
    "from copy import deepcopy\n",
    "\n",
    "MAX_DEFERABLES = 3\n",
    "MPP = 4\n",
    "TICK_LENGTH = 300 / 60\n",
    "GAMMA = 0.99\n",
    "STACKED_NUM = 10\n",
    "MAX_FLYWHEEL_CAPACITY = 50\n",
    "MAX_IMPORT_EXPORT = 50\n",
    "\n",
    "STATE_SIZE = MAX_DEFERABLES * 3 + 1 + 5 + STACKED_NUM * 3 # 1 for flywheel_amt, 5 for cur tick info, 3 for each past tick\n",
    "ACTION_SIZE = 1 + 1 + MAX_DEFERABLES # 1 for buy/sell, 1 for store/release, 1 for each deferable\n",
    "\n",
    "# Penalties\n",
    "EXCEED_FLYWHEEL_PENALTY = 0\n",
    "NEGATIVE_ALLOCATION_PENALTY = 0\n",
    "OVER_ALLOCATION_PENALTY = 0\n",
    "NEGATIVE_ENERGY_PENALTY = 0\n",
    "\n",
    "\n",
    "\n",
    "def simulate_day_naive(day, ticks, use_flywheel=False, export_extra=False):\n",
    "    costs = []\n",
    "    flywheel_amt = 0\n",
    "    for tick in ticks:\n",
    "        power = (tick.sun / 100) * MPP \n",
    "        sun_energy = power * TICK_LENGTH # sun in percentage to joules per tick\n",
    "        \n",
    "        total_energy = sun_energy\n",
    "\n",
    "        if use_flywheel:\n",
    "            total_energy += flywheel_amt\n",
    "            flywheel_amt = 0\n",
    "\n",
    "        total_energy -= tick.demand\n",
    "\n",
    "        for deferable in day.deferables:\n",
    "            if deferable.start == tick.tick:\n",
    "                total_energy -= deferable.energy\n",
    "        \n",
    "        if total_energy < 0:\n",
    "            cost = -total_energy * tick.sell_price\n",
    "        elif use_flywheel:\n",
    "            flywheel_amt = min(MAX_FLYWHEEL_CAPACITY, flywheel_amt + total_energy)\n",
    "        elif export_extra: \n",
    "            cost = total_energy * tick.buy_price # Export extra energy\n",
    "        \n",
    "        costs.append(cost)\n",
    "    \n",
    "    return sum(costs)\n",
    "\n",
    "def tick_to_vect(tick):\n",
    "    return [tick.sun, tick.buy_price, tick.sell_price]\n",
    "\n",
    "def tick_to_history(i, ticks, yest_ticks, STACKED_NUM):\n",
    "    tick: Tick = ticks[i]\n",
    "    cur = [tick.tick, tick.demand, tick.sun, tick.buy_price, tick.sell_price]\n",
    "    for j in range(1, STACKED_NUM + 1):\n",
    "        if i - j < 0 and abs(i - j) <= len(yest_ticks):\n",
    "            prev = yest_ticks[i - j]\n",
    "            cur.extend(tick_to_vect(prev))\n",
    "            # cur.extend([0, 0, 0])\n",
    "        elif i - j >= 0:\n",
    "            prev = ticks[i - j]\n",
    "            cur.extend(tick_to_vect(prev))\n",
    "        else:\n",
    "            cur.extend([0, 0, 0])\n",
    "\n",
    "    return cur\n",
    "\n",
    "# PENALTIES IN ACTION: Flywheel store/release invalid, Negative/over allocation\n",
    "\n",
    "def get_sun_energy(tick, MPP):\n",
    "    return (tick.sun / 100) * MPP * TICK_LENGTH\n",
    "\n",
    "def update_deferable_demands(day_state, action, tick):\n",
    "    deferables = day_state[\"deferables\"]\n",
    "    energy_spent = 0\n",
    "    penalty = 0\n",
    "    for i in range(len(deferables)):\n",
    "        d = deferables[i]\n",
    "        if d.start > tick.tick:\n",
    "            continue\n",
    "        \n",
    "        allocation = action[i + 2].item()\n",
    "        # print(f\"D{i}:\", round(d.energy, 3), \"A:\", round(allocation, 3))\n",
    "        if d.end == tick.tick and d.energy > 0:\n",
    "            energy_spent += d.energy\n",
    "            d.energy = 0\n",
    "        \n",
    "        if allocation < 0:\n",
    "            penalty += -allocation * NEGATIVE_ALLOCATION_PENALTY\n",
    "            continue\n",
    "        elif allocation > d.energy:\n",
    "            energy_spent += d.energy\n",
    "            penalty += (allocation - d.energy) * OVER_ALLOCATION_PENALTY\n",
    "        else:\n",
    "            energy_spent += allocation\n",
    "            d.energy -= allocation\n",
    "\n",
    "    return energy_spent, penalty\n",
    "\n",
    "def buy_sell_to_cost(imp_exp_amt, tick):\n",
    "    if imp_exp_amt < 0:\n",
    "        return imp_exp_amt * tick.buy_price\n",
    "    else:\n",
    "        return imp_exp_amt * tick.sell_price\n",
    "\n",
    "def update_flywheel_amt(day_state, release_store_amt):\n",
    "    cur_flywheel_amt = day_state[\"flywheel_amt\"]\n",
    "    penalty = 0\n",
    "\n",
    "    # If release amount > flywheel amount, set release amount to flywheel amount and penalise\n",
    "    if release_store_amt > cur_flywheel_amt:\n",
    "        penalty += (release_store_amt - cur_flywheel_amt) * EXCEED_FLYWHEEL_PENALTY\n",
    "        release_store_amt = cur_flywheel_amt\n",
    "        \n",
    "    # Else if store amount + cur_flywheel_amt > capacity, set release_store_amt to capacity - cur_flywheel_amt and penalise\n",
    "    elif release_store_amt < 0 and -release_store_amt + cur_flywheel_amt > MAX_FLYWHEEL_CAPACITY:\n",
    "        penalty += (-release_store_amt + cur_flywheel_amt - MAX_FLYWHEEL_CAPACITY) * EXCEED_FLYWHEEL_PENALTY\n",
    "        release_store_amt = MAX_FLYWHEEL_CAPACITY - cur_flywheel_amt\n",
    "    \n",
    "    # If release_store > 0, draw energy and subtract from flywheel_amt\n",
    "    day_state[\"flywheel_amt\"] -= release_store_amt\n",
    "    \n",
    "    return release_store_amt, penalty\n",
    "\n",
    "def environment_step(action, tick, day_state, print_info=False):\n",
    "    total_penalty = 0\n",
    "    \n",
    "    # 1. Get total sun energy\n",
    "    sun_energy = get_sun_energy(tick, MPP)\n",
    "    total_energy = sun_energy\n",
    "\n",
    "    # 2. Get energy bought/sold\n",
    "    imp_exp_amt = action[0].item()\n",
    "    total_energy += imp_exp_amt\n",
    "\n",
    "    # 3. Get energy stored/released\n",
    "    release_store_amt = action[1].item()\n",
    "    release_store_amt, penalty = update_flywheel_amt(day_state, release_store_amt)\n",
    "    total_penalty += penalty\n",
    "    total_energy += release_store_amt\n",
    "\n",
    "    # 4. Satisfy instantaneous demand\n",
    "    total_energy -= tick.demand\n",
    "\n",
    "    # 5. Satisfy deferable demands\n",
    "    energy_spent, penalty = update_deferable_demands(day_state, action, tick)\n",
    "    total_penalty += penalty\n",
    "    total_energy -= energy_spent\n",
    "\n",
    "    # 6. Buy more energy if total energy < 0\n",
    "    if total_energy < 0:\n",
    "        imp_exp_amt += -total_energy\n",
    "    \n",
    "    # todo: Limit max import / export amount\n",
    "    if imp_exp_amt > MAX_IMPORT_EXPORT:\n",
    "        imp_exp_amt = MAX_IMPORT_EXPORT\n",
    "    elif imp_exp_amt < -MAX_IMPORT_EXPORT:\n",
    "        imp_exp_amt = -MAX_IMPORT_EXPORT\n",
    "\n",
    "    cost = buy_sell_to_cost(imp_exp_amt, tick)\n",
    "    \n",
    "    # TODO: Save rest of energy in flywheel \n",
    "    if total_energy > 0:\n",
    "        day_state[\"flywheel_amt\"] = min(MAX_FLYWHEEL_CAPACITY, day_state[\"flywheel_amt\"] + total_energy)\n",
    "\n",
    "    # TODO: OR sell energy\n",
    "\n",
    "    if print_info:\n",
    "        print(\"Tick: \", tick.tick)\n",
    "        print(\"Sun E: \", round(sun_energy, 3))\n",
    "        print(\"Imp/Exp: \", round(imp_exp_amt, 3))\n",
    "        print(\"Cost: \", round(cost, 5))\n",
    "        print(\"Rel/Sto: \", round(release_store_amt, 3))\n",
    "        print(\"Ins D: \", round(tick.demand, 3))\n",
    "        print(\"Def D: \", round(energy_spent, 3))\n",
    "        print(\"E left: \", round(total_energy, 3))\n",
    "        print(\"Penalty: \", round(total_penalty, 3))\n",
    "        print('-' * 20)\n",
    "        print()\n",
    "\n",
    "    return cost, total_penalty\n",
    "\n",
    "def run_simulation(ticks, yest_ticks, day, print_info=False):\n",
    "\n",
    "    day_state = {\n",
    "        \"deferables\": deepcopy(day.deferables),\n",
    "        \"flywheel_amt\": 0\n",
    "    }\n",
    "\n",
    "    # print(\"Deferables: \", [[d.start, d.end] for d in day_state[\"deferables\"]])\n",
    "\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "\n",
    "    total_penalty = 0\n",
    "    total_cost = 0\n",
    "    costs = []\n",
    "    penalties = []\n",
    "    for i, tick in enumerate(ticks):\n",
    "        state = []\n",
    "\n",
    "        # Add deferable demand info\n",
    "        deferables = day_state[\"deferables\"]\n",
    "        for j in range(len(deferables)):\n",
    "            d = deferables[j]\n",
    "            state.extend([d.energy, d.start, d.end])\n",
    "        \n",
    "        # Add store/release info\n",
    "        state.append(day_state[\"flywheel_amt\"])\n",
    "\n",
    "        # Add history\n",
    "        state.extend(tick_to_history(i, ticks, yest_ticks, STACKED_NUM))\n",
    "\n",
    "        # Run policy network\n",
    "        action, log_prob = policy_network.get_action(torch.tensor(state))\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Environment step\n",
    "        cost, penalty = environment_step(action, tick, day_state, print_info=print_info)\n",
    "        # total_penalty += penalty\n",
    "        penalties.append(penalty)\n",
    "        costs.append(cost)\n",
    "        rewards.append(-(cost + penalty))\n",
    "        states.append(state)\n",
    "\n",
    "\n",
    "    return log_probs, rewards, costs, penalties, states\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    rewards = rewards - rewards.mean()\n",
    "\n",
    "    returns = []\n",
    "    R = 0\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    mean = returns.mean()\n",
    "    std = returns.std()\n",
    "\n",
    "    returns = (returns - mean) / (std + 1e-8)  # Normalize returns\n",
    "\n",
    "    return returns\n",
    "\n",
    "def compute_loss(log_probs, returns):\n",
    "    returns = torch.tensor(returns)\n",
    "    loss = 0\n",
    "    for log_prob, G in zip(log_probs, returns):\n",
    "        loss += -log_prob * G\n",
    "    return loss\n",
    "\n",
    "def predict(day, ticks, yest_ticks):\n",
    "    log_probs, rewards, costs, penalties, states = run_simulation(ticks, yest_ticks, day)\n",
    "    loss = compute_loss(log_probs, compute_returns(rewards))\n",
    "\n",
    "    return {\n",
    "        \"reward\": sum(rewards),\n",
    "        \"cost\": sum(costs),\n",
    "        \"penalty\": sum(penalties),\n",
    "        \"loss\": loss\n",
    "    }\n",
    "\n",
    "RUNS_PER_TRAJECTORY = 10\n",
    "def simulate_day(day, ticks, yest_ticks, filename, print_info=False):\n",
    "    global max_returns\n",
    "    \n",
    "    # Run simulation\n",
    "    total_loss = 0\n",
    "    trajectories = []\n",
    "    for i in range(RUNS_PER_TRAJECTORY):\n",
    "        log_probs, rewards, costs, penalties, states = run_simulation(ticks, yest_ticks, day, print_info=print_info)\n",
    "        trajectories.append((sum(rewards), sum(costs), sum(penalties)))\n",
    "        returns = compute_returns(rewards)\n",
    "        total_loss += compute_loss(log_probs, returns)\n",
    "\n",
    "    # Taking only return of first tick \n",
    "        \n",
    "\n",
    "    average_loss = total_loss / RUNS_PER_TRAJECTORY\n",
    "    policy_network.optimizer.zero_grad()\n",
    "    average_loss.backward()\n",
    "    policy_network.optimizer.step()\n",
    "\n",
    "    return {\n",
    "        \"loss\": average_loss.item(),\n",
    "        \"reward\": sum([r[0] for r in trajectories]) / len(trajectories),\n",
    "        \"cost\": sum([r[1] for r in trajectories]) / len(trajectories),\n",
    "        \"penalty\": sum([r[2] for r in trajectories]) / len(trajectories)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "policy_network = PolicyNetwork(STATE_SIZE, ACTION_SIZE)\n",
    "day, yest_ticks = getTicksForDay(0)\n",
    "naive_costs = []\n",
    "naive_with_flywheel = []\n",
    "naive_export_extra = []\n",
    "results = []\n",
    "\n",
    "\n",
    "start = 1\n",
    "num_epochs = 1000\n",
    "filename = f\"checkpoints/pn_epochs{num_epochs}_trajruns_{RUNS_PER_TRAJECTORY}_ema100.pth\"\n",
    "start_time = time.time()\n",
    "min_cost = 1e6\n",
    "last_checkpoint = None\n",
    "min_index = 0\n",
    "for i in range(start, start + num_epochs):\n",
    "    day, ticks = getTicksForDay(i)\n",
    "    naive_costs.append(simulate_day_naive(day, ticks))\n",
    "    naive_with_flywheel.append(simulate_day_naive(day, ticks, use_flywheel=True))\n",
    "    naive_export_extra.append(simulate_day_naive(day, ticks, export_extra=True))\n",
    "\n",
    "    last_checkpoint = {\n",
    "        \"mean_net\": policy_network.mean_net.state_dict().copy(),\n",
    "        \"logstd\": policy_network.logstd.clone()\n",
    "    }\n",
    "\n",
    "    r = simulate_day(day, ticks, yest_ticks, print_info=False, filename=filename)\n",
    "    results.append(r)\n",
    "    emas = get_ema([r[\"cost\"] for r in results], 100)\n",
    "    yest_ticks = ticks\n",
    "\n",
    "    if last_checkpoint != None and emas[-1] < min_cost:\n",
    "        min_index = i\n",
    "        torch.save(last_checkpoint, filename)\n",
    "        \n",
    "\n",
    "    if i % 100 == 0:\n",
    "        cur_time = time.time()\n",
    "        print(f\"Iteration {i}\")\n",
    "        # print(\"Reward\", round(r[\"reward\"], 3))\n",
    "        # print(\"Pen:\", round(r[\"penalty\"], 3))\n",
    "        print(\"Cost:\", round(r[\"cost\"], 3))\n",
    "        print(\"EMA (100): \", emas[-1])\n",
    "        print(f\"Time taken: {round(cur_time - start_time, 2)}s\")\n",
    "        print('-' * 20)\n",
    "        print()\n",
    "\n",
    "    yest_ticks = ticks\n",
    "\n",
    "\n",
    "costs = [r[\"cost\"] for r in results]\n",
    "ema_costs = get_ema(costs, 100)\n",
    "min_cost = np.min(ema_costs)\n",
    "min_epoch = np.argmin(ema_costs)\n",
    "print(f\"Min cost: {round(min_cost, 3)} at epoch {min_epoch}\")\n",
    "\n",
    "plt.plot(ema_costs, label=\"RL\")\n",
    "# plt.plot(get_ema(naive_costs, 100), label=\"Naive\")\n",
    "# plt.plot(get_ema(naive_with_flywheel, 100), label=\"Naive (With FW)\")\n",
    "# plt.plot(get_ema(naive_export_extra, 100), label=\"Naive (Exp Extra)\")\n",
    "plt.xlabel(\"Epochs (days)\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Training cost over days (ema=100)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'server'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m policy_network\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mserver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimisation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ema\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'server'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from policy import PolicyNetwork\n",
    "\n",
    "def load_policy_network_checkpoint(filename):\n",
    "    checkpoint = torch.load(\"checkpoints/\" + filename + '.pth')\n",
    "    policy_network = PolicyNetwork(STATE_SIZE, ACTION_SIZE)\n",
    "    policy_network.mean_net.load_state_dict(checkpoint['mean_net'])\n",
    "    policy_network.logstd = checkpoint['logstd']\n",
    "    return policy_network\n",
    "\n",
    "\n",
    "# Validation\n",
    "from utils import get_ema\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import pyperclip\n",
    "\n",
    "policy_network = load_policy_network_checkpoint(\"pn_epochs1000_trajruns_10_ema100\")\n",
    "start = 100000\n",
    "size = 1000\n",
    "day, ticks = getTicksForDay(start)\n",
    "_, yest_ticks = getTicksForDay(start - 1)\n",
    "\n",
    "costs = []\n",
    "naive_store_fw = []\n",
    "naive_export_extra = []\n",
    "\n",
    "total = 0\n",
    "for i in range(start + 1, start + size + 1):\n",
    "    day, new_ticks = getTicksForDay(i)\n",
    "    start_time = time.time()\n",
    "    r = predict(day, new_ticks, yest_ticks)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total += end_time - start_time\n",
    "    costs.append(r[\"cost\"])\n",
    "    naive_store_fw.append(simulate_day_naive(day, new_ticks, use_flywheel=True))\n",
    "    naive_export_extra.append(simulate_day_naive(day, new_ticks, export_extra=True))\n",
    "    yest_ticks = ticks\n",
    "\n",
    "    if i % 250 == 0:\n",
    "        print(f\"Iteration {i}\")\n",
    "        print(\"Cost:\", round(r[\"cost\"], 3))\n",
    "        emas = get_ema(costs, 100)\n",
    "        print(\"EMA Cost:\", round(emas[-1], 3))\n",
    "        print('-' * 20)\n",
    "\n",
    "print(\"Average time taken: \", total / (size * 60))\n",
    "\n",
    "table_md = \"| Algorithm | Average Cost (per day) |\\n\"\n",
    "table_md += \"|-----------|------------------------|\\n\"\n",
    "table_md += f\"| RL | {round(np.mean(costs), 2)} |\\n\"\n",
    "table_md += f\"| Naive FW | {round(np.mean(naive_store_fw), 2)} |\\n\"\n",
    "table_md += f\"| Naive EE | {round(np.mean(naive_export_extra), 2)} |\\n\"\n",
    "pyperclip.copy(table_md)\n",
    "print(\"Table copied to clipboard\")\n",
    "display(Markdown(table_md))\n",
    "\n",
    "plt.plot(get_ema(costs, 100), label=\"RL\")\n",
    "plt.plot(get_ema(naive_store_fw, 100), label=\"Naive (With FW)\")\n",
    "plt.plot(get_ema(naive_export_extra, 100), label=\"Naive (Exp Extra)\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost over days\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Potential Improvements:\n",
    "\n",
    "Incentivize energy efficiency by rewarding the system for maintaining a balance between energy produced, stored, and consumed.\n",
    "efficiency_reward = -abs(total_energy)  # Penalize large deviations in total energy balance\n",
    "reward = -(cost + total_penalty) + efficiency_reward\n",
    "\n",
    "Stability (Flywheel)\n",
    "optimal_flywheel_amt = some_value\n",
    "stability_reward = -abs(day_state[\"flywheel_amt\"] - optimal_flywheel_amt)  # Penalize deviations from the optimal level\n",
    "reward = -(cost + total_penalty) + stability_reward\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def exponential_moving_average(data, alpha):\n",
    "#     ema = [data[0]]  # Start with the first value\n",
    "#     for i in range(1, len(data)):\n",
    "#         ema.append(alpha * data[i] + (1 - alpha) * ema[-1])\n",
    "#     return ema\n",
    "\n",
    "\n",
    "# N = 20  # Number of periods for EMA\n",
    "# alpha = 2 / (N + 1)\n",
    "# averaged_costs = exponential_moving_average(total_costs, alpha)\n",
    "# plt.plot(averaged_costs)\n",
    "# plt.xlabel('Day')\n",
    "# plt.ylabel('EMA Loss')\n",
    "# plt.title('Exponential Moving Average Loss Over Time')\n",
    "# plt.show()\n",
    "\n",
    "# # print(costs)\n",
    "# # decrease = abs(naive_costs - costs) * 100 / naive_costs\n",
    "# # print(\"Naive: \", round(naive_costs, 2), \"\\tRL: \", round(costs, 2), \"\\tpercentage decrease: \", decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def state_to_total_energy(action, tick, day_state, print_info=False):\n",
    "#     power = (tick.sun / 100) * MPP \n",
    "#     sun_energy = power * TICK_LENGTH # sun in percentage to joules per tick\n",
    "    \n",
    "#     energy = sun_energy\n",
    "\n",
    "#     buy_sell_amount = action[0].item()\n",
    "#     energy += buy_sell_amount\n",
    "\n",
    "#     # Add flywheel release / store to total energy and update flywheel amount\n",
    "#     store_release_amount = action[1].item()\n",
    "\n",
    "#     penalty = 0\n",
    "#     if store_release_amount > 0 and store_release_amount > MAX_FLYWHEEL_CAPACITY - day_state[\"flywheel_amt\"]:\n",
    "#         penalty += (store_release_amount + day_state[\"flywheel_amt\"]  - MAX_FLYWHEEL_CAPACITY) * EXCEED_FLYWHEEL_PENALTY\n",
    "#         store_release_amount = day_state[\"flywheel_amt\"]\n",
    "\n",
    "#     if store_release_amount < -MAX_FLYWHEEL_CAPACITY:\n",
    "#         store_release_amount = -MAX_FLYWHEEL_CAPACITY\n",
    "    \n",
    "#     day_state[\"flywheel_amt\"] += store_release_amount\n",
    "#     energy += store_release_amount\n",
    "\n",
    "#     if energy < 0:\n",
    "#         buy_sell_amount += -energy\n",
    "#         energy = 0\n",
    "    \n",
    "#     if print_info:\n",
    "#         print(\"Sun energy: \", round(sun_energy, 3))\n",
    "#         print(\"Flywheel Amount: \", round(day_state[\"flywheel_amt\"], 3))\n",
    "#         print(\"Release/Store: \", round(store_release_amount, 3))\n",
    "#         print(\"Buy/Sell: \", round(buy_sell_amount, 3))\n",
    "#         print(\"Total Energy: \", round(energy, 3))\n",
    "\n",
    "#     return energy, buy_sell_amount, store_release_amount, penalty\n",
    "\n",
    "# def environment_step(action, tick, day_state, print_info=False):\n",
    "\n",
    "#     # Update flywheel amount\n",
    "#     total_energy, buy_sell_amount, store_release_amount, penalty = state_to_total_energy(action, tick, day_state, print_info=print_info)\n",
    "\n",
    "#     # Spend energy on inst demand \n",
    "#     total_energy_spent = tick.demand # demand in joules\n",
    "\n",
    "#     if print_info:\n",
    "#         print(\"Inst demand:\", round(tick.demand, 3))\n",
    "#         print(\"DD before:\", [round(d.energy, 3) for d in day_state[\"deferables\"]])\n",
    "    \n",
    "#     # Update deferable_demands\n",
    "#     deferables = day_state[\"deferables\"]\n",
    "#     allocation = []\n",
    "\n",
    "#     for i in range(len(deferables)):\n",
    "\n",
    "#         if deferables[i].start > tick.tick:\n",
    "#             continue\n",
    "\n",
    "#         energy_demand = deferables[i].energy\n",
    "#         energy_allocated = max(0, action[i + 2].item())\n",
    "#         allocation.append(energy_allocated)\n",
    "#         deferables[i].energy = max(0, energy_demand - energy_allocated) # Update deferable energy\n",
    "#         total_energy_spent += energy_allocated # Update total energy spent\n",
    "        \n",
    "#         # Spend energy to finish deferable demand if deadline is reached\n",
    "#         if deferables[i].end == tick.tick and deferables[i].energy != 0:\n",
    "#             total_energy_spent += deferables[i].energy\n",
    "#             deferables[i].energy = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Buy energy if total energy spent is greater than total energy\n",
    "#     if total_energy_spent > total_energy:\n",
    "#         buy_sell_amount += total_energy_spent - total_energy\n",
    "    \n",
    "#     # Store excess energy in flywheel\n",
    "#     if total_energy < total_energy_spent:\n",
    "#         day_state[\"flywheel_amt\"] = max(MAX_FLYWHEEL_CAPACITY, day_state[\"flywheel_amt\"] + total_energy - total_energy_spent)\n",
    "    \n",
    "#     cost = buy_sell_amount * tick.buy_price if buy_sell_amount > 0 else buy_sell_amount * tick.sell_price\n",
    "\n",
    "#     if print_info:\n",
    "#         print(\"DD after: \", [round(d.energy, 3) for d in day_state[\"deferables\"]])\n",
    "#         print(\"Allocation:\", [round(a, 3) for a in allocation])\n",
    "#         print(\"TE spent:\", round(total_energy_spent, 3))\n",
    "#         print(\"Buy/Sell final:\", round(buy_sell_amount, 3))\n",
    "#         print(\"Cost: \", round(cost, 2))\n",
    "#         print(\"Penalty: \", penalty)\n",
    "#         print('-'*10)\n",
    "#         print()\n",
    "\n",
    "#     return cost, penalty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartgrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
